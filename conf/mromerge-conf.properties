# 用来配置 merge-seqfile 所需要的配置信息
# 此文件存放在HDFS上
###########################################################################
######################### CDH集群访问的 相关配置 #########################
# CDH 集群使用的安全策略，kerberos 或 simple
hadoop_security_authentication=kerberos
#hadoop_security_authentication=simple
# CDH 集群如果使用kerberos安全策略的话，kerberos的配置文件的位置
#krb5_conf=/etc/krb5.conf
#krb5_conf=C:\\Program Files\\MIT\\Kerberos\\krb5-WY.ini
krb5_conf=G:\\Users\\lkz\\IdeaProjects\\datacollection3\\datacollection(2)\\conf\\krb5.conf
# 访问 CDH Hive 的用户、keytab文件位置
hive_user=mrdas@WY.CMDI.CMCC
#hive_user=hive@WY.CMDI.CMCC
#hive_keytab=C:\\Program Files\\MIT\\Kerberos\\mrdas-WY.keytab
hive_keytab=G:\\Users\\lkz\\IdeaProjects\\datacollection3\\datacollection(2)\\conf\\mrdas-WY.keytab
#hive_keytab=G:\\Users\\lkz\\IdeaProjects\\datacollection3\\datacollection(2)\\conf\\hive.keytab
#hive_keytab=/etc/kerberos/hive.keytab
#mromerge_conf_hdfs=/zhangyan/mro/property/mromerge-conf.properties
mromerge_conf_hdfs=/test/test_lkz_other/prop/lkz-test-mromerge-conf-v5.properties
######################### Sequence Files 相关配置 #########################
# 位于HDFS上的某地市某日期某小时的所有SequenceFile 输出的根目录，其下设 /<city>/<date>/<hour> 三级子目录
#seqfile_root=/zhangyan/mro/sf
seqfile_root=/test/test_lkz_other/bigdata_new
# 位于HDFS上的comp_seqfiles目录用来存放名为 city-date-hour 格式的空文件，
# 用来表明 某地市某日期某小时的所有SequenceFile已完成输出、等待合并
# 根据这个配置，和seqfile_root的值一起拼成要合并的某地市某日期某小时的 SeqFile所在目录
comp_seqfiles=/zhangyan/mro/comp_seqfiles
######################### Hive表相关配置 #########################
#  beeline 连接的用户字符串
beeline_user=jdbc:hive2://could004:10000/
beeline_principal=hive/could004@WY.CMDI.CMCC
# Hive中用来存放合并后的MRO数据的 数据库名
dbname=mro
# Hive中用来存放合并后的MRO数据的 表名、表文件存放路径
tb_name=mro_dt_rec
tb_location=/zhangyan/mro/mro_dt_rec
# Hive中用来存放合并后的MRO数据的 表 的压缩算法，默认为 "snappy"
tb_compress=snappy
# 描述日志的字段信息（作为构造Hive表的列的依据）文件所在位置
#mroobj_json=/zhangyan/mro/property/mro_field.json
mroobj_json=/lkz/test/conf/mro_field_new.json
#mroobj_json=/lkz/test/conf/mro_field_20190902_int.json
# Hive表中数据保留期限
data_valid_days=7
######################### 数据合并程序 产生的log的 相关配置 #########################
# merge-data.sh产生日志的目录
#log_dir=/home/aspire/zhangyan/datacollection/logs/
log_dir=D:\\Workspace\\bigdata\\code\\trunk\\datacollection\\merge-seqfile\\logs
# Spark merge-seqfile的日志级别，可选值 ERROR、INFO、DEBUG、TRACE
log_level=INFO
# 日志数据库访问方式
# could008
pgdb_ip=10.254.222.226
pgdb_port=5432
pgdb_user=zhangyan
pgdb_passwd=kKRLVHNfR554Ihda6uHhRg==
pgdb_db=mlogdb

